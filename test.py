from datasets import load_dataset
from transformers import AutoTokenizer, AutoConfig
from modelingmoe import BGEM3Model
import torch
import numpy as np
import os

# âœ… GPU ì„¤ì •
device = "cuda:5" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# âœ… ëª¨ë¸ ê²½ë¡œ
model_path = "/home/sunga/Desktop/BGE-M3-CLP-MoE-main/outputs/korean/"
model_name = "bge-m3-moe-korean"

# âœ… í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained(model_path)
config = AutoConfig.from_pretrained(model_path)
model = BGEM3Model(
    model_name=model_path,
    colbert_dim=1024,
    moe="intermediate",
    only_train="intermediate",
    num_experts=2,
    num_experts_per_tok=1
).to(device)
model.eval()

# âœ… ë¬¸ì„œ ë¡œë”©
corpus = load_dataset("castorini/mr-tydi-corpus", "korean", split="train")

# âœ… docid_to_text ìƒì„±
docid_to_text = {}
for doc in corpus:
    text = doc.get("title", "") + " " + doc.get("text", "")
    if text.strip():
        docid_to_text[doc["docid"]] = text

# âœ… ì •ë‹µ ì¿¼ë¦¬ì™€ ë¬¸ì„œ
query = "í•œë‹ˆë°œ ë°”ë¥´ì¹´ì˜ ìµœì¢… ê³„ê¸‰ì€ ë¬´ì—‡ì¸ê°€ìš”?"
docid = "7207#0"

if docid not in docid_to_text:
    print(f"âŒ ì •ë‹µ ë¬¸ì„œ {docid} ê°€ corpusì— ì—†ìŠµë‹ˆë‹¤.")
    exit()

doc_text = docid_to_text[docid]

# âœ… ì„ë² ë”© í•¨ìˆ˜
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(device)
    with torch.no_grad():
        dense_vec, _, _ = model.encode(inputs)
        return dense_vec.cpu().numpy().flatten()

# âœ… ìœ ì‚¬ë„ ê³„ì‚°
query_vec = get_embedding("Query: " + query)
doc_vec = get_embedding(doc_text)
cos_sim = np.dot(query_vec, doc_vec)

print(f"\nğŸ§ª Cosine similarity between query and gold doc ({docid}): {cos_sim:.4f}")
